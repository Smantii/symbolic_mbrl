{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3e5422-8528-4ab9-8495-efe23fb9e0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import mbrl.models as models\n",
    "from mbrl.models import Model\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "from pyoperon.sklearn import SymbolicRegressor\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "82e751ff-3f48-49a4-b102-bdd60e6554f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_fn(a, next_obs):\n",
    "    #threshold = 10\n",
    "    #not_done = (next_obs > -threshold)* (next_obs < threshold)\n",
    "    #done = ~not_done\n",
    "    #done = done[:, None]\n",
    "    return False\n",
    "\n",
    "def reward_fn(a, next_obs):\n",
    "    return torch.cos(2 * torch.pi * next_obs) * torch.exp(torch.abs(next_obs) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bcf1bf24-6afd-45dd-9c2d-0d963240493c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Simple1DMDP(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Simple1DMDP, self).__init__()\n",
    "        \n",
    "        # define the action space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # define the observation space: continuous single dimension for position\n",
    "        self.observation_space = spaces.Box(low=-10, high=10, shape=(1,), dtype=np.float32)\n",
    "        # Initialize state and episode length\n",
    "        self.state = 0.0\n",
    "        self.episode_length = 10\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        # reset the state to 0 and the step counter\n",
    "        self.state = 0.0\n",
    "        self.current_step = 0\n",
    "        return torch.FloatTensor([self.state]), {}\n",
    "\n",
    "    def step(self, action):  \n",
    "        # update state based on action\n",
    "        self.state += action\n",
    "        # clip in the case we go outside of [-10,10]\n",
    "        actual_new_state = self.state\n",
    "        self.state = np.clip(self.state, -10.,10.)\n",
    "        \n",
    "        # calculate reward\n",
    "        #reward = torch.cos(2 * torch.pi * self.state) * torch.exp(torch.abs(self.state) / 3)\n",
    "        reward = reward_fn(torch.from_numpy(action), torch.from_numpy(self.state))\n",
    "        # increment step counter\n",
    "        self.current_step += 1\n",
    "        # check if episode is terminated\n",
    "        #terminated = term_fn(action, actual_new_state)\n",
    "        terminated = self.current_step >= self.episode_length\n",
    "        # set placeholder for truncated\n",
    "        truncated = False\n",
    "        # set placeholder for info\n",
    "        info = {}\n",
    "        return torch.FloatTensor([self.state]).flatten(), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # simple print rendering\n",
    "        print(f\"Step: {self.current_step}, State: {self.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "40e84fe0-55a9-4f85-9713-7cb10a595c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SymbolicModel(Model):\n",
    "    def __init__(self, symbols, population_size, generations, max_length):\n",
    "        super().__init__(\"cpu\")\n",
    "        self.reg = SymbolicRegressor(population_size = population_size,\n",
    "                                     allowed_symbols=symbols,\n",
    "                                     optimizer_iterations=10,\n",
    "                                     generations = generations,\n",
    "                                     n_threads=32,\n",
    "                                     max_evaluations=int(1e6),\n",
    "                                     max_length = max_length,\n",
    "                                     tournament_size = 3)\n",
    "    def forward(self,x):\n",
    "        return self.reg.predict(x), None\n",
    "    \n",
    "    def loss(self, model_in, target):\n",
    "        return self.reg.score(model_in, target)\n",
    "    \n",
    "    def eval_score(self, model_in, target):\n",
    "        return self.reg.score(model_in, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9748292f-29b1-4727-bbc3-d38a6a69e2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (tensor([0.]), {})\n",
      "Action: [0.21400476], State: tensor([0.2140]), Reward: tensor([0.2408])\n",
      "Step: 1, State: [0.21400476]\n",
      "Action: [-0.55617166], State: tensor([-0.3422]), Reward: tensor([-0.6134])\n",
      "Step: 2, State: [-0.3421669]\n",
      "Action: [0.30529296], State: tensor([-0.0369]), Reward: tensor([0.9853])\n",
      "Step: 3, State: [-0.03687394]\n",
      "Action: [-0.5669092], State: tensor([-0.6038]), Reward: tensor([-0.9720])\n",
      "Step: 4, State: [-0.60378313]\n",
      "Action: [-0.28346726], State: tensor([-0.8873]), Reward: tensor([1.0207])\n",
      "Step: 5, State: [-0.8872504]\n",
      "Action: [0.23664586], State: tensor([-0.6506]), Reward: tensor([-0.7263])\n",
      "Step: 6, State: [-0.65060455]\n",
      "Action: [0.15619661], State: tensor([-0.4944]), Reward: tensor([-1.1784])\n",
      "Step: 7, State: [-0.49440795]\n",
      "Action: [0.34170285], State: tensor([-0.1527]), Reward: tensor([0.6039])\n",
      "Step: 8, State: [-0.1527051]\n",
      "Action: [0.8935186], State: tensor([0.7408]), Reward: tensor([-0.0738])\n",
      "Step: 9, State: [0.7408135]\n",
      "Action: [0.39532644], State: tensor([1.1361]), Reward: tensor([0.9579])\n",
      "Step: 10, State: [1.1361399]\n"
     ]
    }
   ],
   "source": [
    "# Register the custom environment\n",
    "gym.envs.registration.register(\n",
    "    id='Simple1DMDP-v0',\n",
    "    entry_point=Simple1DMDP\n",
    ")\n",
    "\n",
    "\n",
    "env = gym.make('Simple1DMDP-v0')\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "print(f\"Initial State: {state}\")\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    print(f\"Action: {action}, State: {state}, Reward: {reward}\")\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "60d27669-573d-4891-91cf-b6730febed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "env.reset()\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8de040d-2a44-4738-8483-8d994882c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 500\n",
    "num_trials = 10\n",
    "ensemble_size = 7\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 4,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": True,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.SiLU\"}\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": True,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 256,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0be85b89-bfd6-4474-871d-5dfcd756f35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "672ae78a-7b4d-42d9-9248-b72de32915fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e456e130-9bf0-41c2-aa89-24983822ed5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples stored 500\n"
     ]
    }
   ],
   "source": [
    "common_util.rollout_agent_trajectories(\n",
    "    env,\n",
    "    trial_length, # initial exploration steps\n",
    "    planning.RandomAgent(env),\n",
    "    {}, # keyword arguments to pass to agent.act()\n",
    "    replay_buffer=replay_buffer,\n",
    "    trial_length=trial_length\n",
    ")\n",
    "\n",
    "print(\"# samples stored\", replay_buffer.num_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f16bc442-e92a-4660-a4e3-f4d377093d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_cfg = omegaconf.OmegaConf.create({\n",
    "    # this class evaluates many trajectories and picks the best one\n",
    "    \"_target_\": \"mbrl.planning.TrajectoryOptimizerAgent\",\n",
    "    \"planning_horizon\": 15,\n",
    "    \"replan_freq\": 1,\n",
    "    \"verbose\": False,\n",
    "    \"action_lb\": \"???\",\n",
    "    \"action_ub\": \"???\",\n",
    "    # this is the optimizer to generate and choose a trajectory\n",
    "    \"optimizer_cfg\": {\n",
    "        \"_target_\": \"mbrl.planning.CEMOptimizer\",\n",
    "        \"num_iterations\": 5,\n",
    "        \"elite_ratio\": 0.1,\n",
    "        \"population_size\": 350,\n",
    "        \"alpha\": 0.1,\n",
    "        \"device\": device,\n",
    "        \"lower_bound\": \"???\",\n",
    "        \"upper_bound\": \"???\",\n",
    "        \"return_mean_elites\": True,\n",
    "        \"clipped_normal\": False\n",
    "    }\n",
    "})\n",
    "\n",
    "agent = planning.create_trajectory_optim_agent_for_model(\n",
    "    model_env,\n",
    "    agent_cfg,\n",
    "    num_particles=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9008cea9-6472-4a14-935e-5acaba27a3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "def train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n",
    "    train_losses.append(tr_loss)\n",
    "    val_scores.append(val_score.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24b7dd8d-d18e-42d8-89b9-b21b173fe757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# Create a trainer for the model\n",
    "model_trainer = models.ModelTrainer(dynamics_model, optim_lr=7.5e-4, weight_decay=3e-5)\n",
    "\n",
    "    \n",
    "# Main PETS loop\n",
    "all_rewards = [0]\n",
    "for trial in range(num_trials):\n",
    "    obs, _ = env.reset()    \n",
    "    agent.reset()\n",
    "    \n",
    "    terminated = False\n",
    "    total_reward = 0.0\n",
    "    steps_trial = 0\n",
    "    while not terminated:\n",
    "        # --------------- Model Training -----------------\n",
    "        if steps_trial == 0:\n",
    "            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n",
    "            \n",
    "            print(replay_buffer.reward.shape)\n",
    "            \n",
    "            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n",
    "                replay_buffer,\n",
    "                batch_size=cfg.overrides.model_batch_size,\n",
    "                val_ratio=cfg.overrides.validation_ratio,\n",
    "                ensemble_size=ensemble_size,\n",
    "                shuffle_each_epoch=True,\n",
    "                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n",
    "            )\n",
    "                \n",
    "            model_trainer.train(\n",
    "                dataset_train, \n",
    "                dataset_val=dataset_val, \n",
    "                num_epochs=2000, \n",
    "                patience=25, \n",
    "                callback=train_callback,\n",
    "                silent=True)\n",
    "            \n",
    "\n",
    "        # --- Doing env step using the agent and adding to model dataset ---\n",
    "        next_obs, reward, terminated, truncated, _ = common_util.step_env_and_add_to_buffer(\n",
    "            env, obs, agent, {}, replay_buffer)\n",
    "            \n",
    "        \n",
    "        \n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_trial += 1\n",
    "        \n",
    "        if steps_trial == trial_length:\n",
    "            break\n",
    "    \n",
    "    all_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "55c7016f-cecf-4db8-9436-86570939acb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1c7da44c-fec1-4def-91cd-640b545e6a96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When using propagation='fixed_model', `propagation_indices` must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m91\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mdynamics_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/symbolic_mbrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/symbolic_mbrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/symbolic_mbrl/lib/python3.10/site-packages/mbrl/models/gaussian_mlp.py:278\u001b[0m, in \u001b[0;36mGaussianMLP.forward\u001b[0;34m(self, x, rng, propagation_indices, use_propagation)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes mean and logvar predictions for the given input.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03mWhen ``self.num_members > 1``, the model supports uncertainty propagation options\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_propagation:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpropagation_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpropagation_indices\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_forward(x)\n",
      "File \u001b[0;32m~/mambaforge/envs/symbolic_mbrl/lib/python3.10/site-packages/mbrl/models/gaussian_mlp.py:209\u001b[0m, in \u001b[0;36mGaussianMLP._forward_ensemble\u001b[0;34m(self, x, rng, propagation_indices)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagation_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfixed_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m propagation_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    210\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using propagation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixed_model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, `propagation_indices` must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_from_indices(x, propagation_indices)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagation_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpectation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: When using propagation='fixed_model', `propagation_indices` must be provided."
     ]
    }
   ],
   "source": [
    "r = torch.from_numpy(np.linspace(-10,10, 91).reshape(-1,1))\n",
    "with torch.no_grad():\n",
    "    dynamics_model.model(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
