{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3e5422-8528-4ab9-8495-efe23fb9e0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import omegaconf\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "import mbrl.models as models\n",
    "from mbrl.models import Model\n",
    "import mbrl.planning as planning\n",
    "import mbrl.util.common as common_util\n",
    "import mbrl.util as util\n",
    "from pyoperon.sklearn import SymbolicRegressor\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "mpl.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf1bf24-6afd-45dd-9c2d-0d963240493c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Simple1DMDP(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Simple1DMDP, self).__init__()\n",
    "        \n",
    "        # define the action space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # define the observation space: continuous single dimension for position\n",
    "        self.observation_space = spaces.Box(low=-10, high=10, shape=(1,), dtype=np.float32)\n",
    "        # Initialize state and episode length\n",
    "        self.state = 0.0\n",
    "        self.episode_length = 10\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        # reset the state to 0 and the step counter\n",
    "        self.state = 0.0\n",
    "        self.current_step = 0\n",
    "        return np.array([self.state], dtype=np.float32), {}\n",
    "\n",
    "    def step(self, action):  \n",
    "        # update state based on action\n",
    "        self.state += action\n",
    "        # clip in the case we go outside of [-10,10]\n",
    "        self.state = np.clip(self.state, -10.,10.)\n",
    "        \n",
    "        # calculate reward\n",
    "        reward = np.cos(2 * np.pi * self.state) * np.exp(np.abs(self.state) / 3)\n",
    "        # increment step counter\n",
    "        self.current_step += 1\n",
    "        # check if episode is terminated\n",
    "        terminated = self.current_step >= self.episode_length\n",
    "        # set placeholder for truncated\n",
    "        truncated = False\n",
    "        # set placeholder for info\n",
    "        info = {}\n",
    "        return np.array([self.state], dtype=np.float32), reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # simple print rendering\n",
    "        print(f\"Step: {self.current_step}, State: {self.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0810e270-58fa-46f9-9845-54afe4ec76f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_fn(a, next_obs):\n",
    "    pass\n",
    "\n",
    "def reward_fn(a, next_obs):\n",
    "    return np.cos(2 * np.pi * next_obs) * np.exp(np.abs(next_obs) / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e84fe0-55a9-4f85-9713-7cb10a595c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SymbolicModel(Model):\n",
    "    def __init__(self, symbols, population_size, generations, max_length):\n",
    "        super().__init__(\"cpu\")\n",
    "        self.reg = SymbolicRegressor(population_size = population_size,\n",
    "                                     allowed_symbols=symbols,\n",
    "                                     optimizer_iterations=10,\n",
    "                                     generations = generations,\n",
    "                                     n_threads=32,\n",
    "                                     max_evaluations=int(1e6),\n",
    "                                     max_length = max_length,\n",
    "                                     tournament_size = 3)\n",
    "    def forward(self,x):\n",
    "        return self.reg.predict(x), None\n",
    "    \n",
    "    def loss(self, model_in, target):\n",
    "        return self.reg.score(model_in, target)\n",
    "    \n",
    "    def eval_score(self, model_in, target):\n",
    "        return self.reg.score(model_in, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9748292f-29b1-4727-bbc3-d38a6a69e2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (array([0.], dtype=float32), {})\n",
      "Action: [-0.81615496], State: [[-0.81615496]], Reward: [0.53004694]\n",
      "Step: 1, State: [-0.81615496]\n",
      "Action: [-0.12825458], State: [[-0.94440955]], Reward: [1.2872664]\n",
      "Step: 2, State: [-0.94440955]\n",
      "Action: [0.96853346], State: [[0.02412391]], Reward: [0.99651563]\n",
      "Step: 3, State: [0.02412391]\n",
      "Action: [0.70285285], State: [[0.72697675]], Reward: [-0.18368363]\n",
      "Step: 4, State: [0.72697675]\n",
      "Action: [-0.29498503], State: [[0.43199173]], Reward: [-1.0510392]\n",
      "Step: 5, State: [0.43199173]\n",
      "Action: [-0.78522635], State: [[-0.35323462]], Reward: [-0.67959213]\n",
      "Step: 6, State: [-0.35323462]\n",
      "Action: [0.2519107], State: [[-0.10132393]], Reward: [0.83172154]\n",
      "Step: 7, State: [-0.10132393]\n",
      "Action: [0.4385841], State: [[0.33726016]], Reward: [-0.5832286]\n",
      "Step: 8, State: [0.33726016]\n",
      "Action: [0.48986024], State: [[0.8271204]], Reward: [0.6137009]\n",
      "Step: 9, State: [0.8271204]\n",
      "Action: [-0.7058208], State: [[0.12129962]], Reward: [0.7532014]\n",
      "Step: 10, State: [0.12129962]\n"
     ]
    }
   ],
   "source": [
    "# Register the custom environment\n",
    "gym.envs.registration.register(\n",
    "    id='Simple1DMDP-v0',\n",
    "    entry_point=Simple1DMDP\n",
    ")\n",
    "\n",
    "\n",
    "env = gym.make('Simple1DMDP-v0')\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "print(f\"Initial State: {state}\")\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    print(f\"Action: {action}, State: {state}, Reward: {reward}\")\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60d27669-573d-4891-91cf-b6730febed6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "env.reset()\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "generator = torch.Generator(device=device)\n",
    "generator.manual_seed(seed)\n",
    "obs_shape = env.observation_space.shape\n",
    "act_shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8de040d-2a44-4738-8483-8d994882c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_length = 500\n",
    "num_trials = 10\n",
    "ensemble_size = 7\n",
    "\n",
    "# Everything with \"???\" indicates an option with a missing value.\n",
    "# Our utility functions will fill in these details using the \n",
    "# environment information\n",
    "cfg_dict = {\n",
    "    # dynamics model configuration\n",
    "    \"dynamics_model\": {\n",
    "        \"_target_\": \"mbrl.models.GaussianMLP\",\n",
    "        \"device\": device,\n",
    "        \"num_layers\": 4,\n",
    "        \"ensemble_size\": ensemble_size,\n",
    "        \"hid_size\": 200,\n",
    "        \"in_size\": \"???\",\n",
    "        \"out_size\": \"???\",\n",
    "        \"deterministic\": True,\n",
    "        \"propagation_method\": \"fixed_model\",\n",
    "        # can also configure activation function for GaussianMLP\n",
    "        \"activation_fn_cfg\": {\n",
    "            \"_target_\": \"torch.nn.SiLU\"}\n",
    "    },\n",
    "    # options for training the dynamics model\n",
    "    \"algorithm\": {\n",
    "        \"learned_rewards\": False,\n",
    "        \"target_is_delta\": True,\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    # these are experiment specific options\n",
    "    \"overrides\": {\n",
    "        \"trial_length\": trial_length,\n",
    "        \"num_steps\": num_trials * trial_length,\n",
    "        \"model_batch_size\": 256,\n",
    "        \"validation_ratio\": 0.05\n",
    "    }\n",
    "}\n",
    "cfg = omegaconf.OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0be85b89-bfd6-4474-871d-5dfcd756f35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 1-D dynamics model for this environment\n",
    "dynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n",
    "\n",
    "# Create a gym-like environment to encapsulate the model\n",
    "model_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
